\section{Mathematical derivation} % {{{
\subsection{Diagonalization of Choi matrix} % {{{

Prueba de la $\sa$ \todo{Alejo escribe esta parte. Dejelo aca todo y luego si acaso 
pasamos cosas al apendice. Caso general}

This part is devoted to derive the set of conditions a Pauli map need to satisfy to fulfill the CP condition i.e. all the eigenvalues of the Choi matrix associated to the channel must be non-negative. First we obtain the set of inequalities for a single qubit Pauli channel and then we extend the results for the general case of $N$ qubits. 

\subsubsection{Single qubit Pauli channels}

\afnote{Notación para matrices densidad vectorizadas? $\varrho$ ?}
\janote{Qué tal $\vec\rho\quad$?}


We now turn to find the matrix form of the map $\mathcal{E}$ in order to construct the Choi matrix and the conditions the set of parameters $\{\tau_{\mu}\}$ need to satisfy in order to $\mathcal{E}$ be considered a quantum channel. By writing the density matrix in vector form $\ket{\rho}=\sum_{\mu=0}^{3} \alpha_{\mu} \ket{\sigma_{\mu}}$, it is easy to show that the matrix form of the map $\mathcal{E}$ may be written as
% 
\begin{equation}
\hat{\mathcal{E}} = \frac{1}{2}\sum_{\mu=0}^{3} \tau_{\mu} \dyad{\sigma_{\mu}},
% \label{Map_Pauli}
\end{equation}
%
where the kets $\ket{\sigma_{\mu}}$ correspond to the vectorized elements of
the Pauli basis and satisfy the usual relation $
\braket{{\sigma_{\mu} }}{{\sigma_{\mu'} }} = \tr \left(\sigma_{\mu}^{\dagger}
\sigma_{\mu'}\right) = 2 ~\delta_{\mu\mu'}$. After some steps it is possible to
show that the associated Choi matrix reads
% 
\begin{equation}
 \mathcal{D} = \frac{1}{2}\sum_{\mu=0}^{3} \tau_{\mu} \sigma_{\mu} \otimes \sigma_{\mu}^*.
%  \label{Choi_1}
\end{equation}
% 
Note that $\mathcal{D}$ has the same form as the matrix representation of a random unitary channel whose Kraus operators are given by $K_{\mu}=\sqrt{\frac{\tau_{\mu}}{2}}\sigma_{\mu}$. For the latter case, it has been shown that corresponding eigenvalues read \cite{CHRUSCINSKI20131425}
% 
\begin{equation}
 \lambda_{\nu} = \frac{1}{2}\sum_{\mu=0}^3H_{\nu\mu}^{(4)}\tau_{\mu},
\end{equation}
%
or
% 
\begin{equation}
 \vec{\lambda} = \frac{1}{2}~ H^{(2)}\otimes H^{(2)}\vec{\tau},
\end{equation}
% 
where $H^{(m)}$ is the Hadamard matrix of order \textit{m}, and for $m=2$ reduces to the usual Hadamard matrix.
% 
\begin{equation}
 H^{(2)}=\begin{pmatrix}
        1 & 1\\
        1 & -1\\
    \end{pmatrix}.
 \label{Hadd_Mat}
\end{equation}
% 
(Proofs in an appendix using binary indices XD)

% By writing the elements of the density matrix in vector form $\varrho=\left(\varrho_0,\varrho_1,\varrho_2,\varrho_3\right)^{\intercal}$, the action of a Pauli map on $\varrho$ for a single qubit may be written as $\varrho \to \varrho'=\hat \mcE \varrho$, where $\hat \mcE$ is the matrix representation of the map. Moreover it can be shown that in the Pauli basis $(\sigma_0,\sigma_1,i\sigma_2,\sigma_3)$, $\hat \mcE$ reduces simply to $\mathrm{diag}\left(\tau_0,\tau_1,\tau_2,\tau_3\right)$. 
% 
% Let reorganize the elements of the Pauli basis as $(\sigma_0,\sigma_3,\sigma_1,i\sigma_2)$, in this way it is possible to write the matrix representation of the map as $\hat \mcE = \tau_{03} \oplus \tau_{12}$, where $\tau_{jk}=\mathrm{diag}\left(\tau_j,\tau_k\right)$ and the symbol $``\oplus "$ represents a direct sum of matrices. The advantage of this ordering is manifested when the matrix $\hat \mcE$ is transformed to the computational basis $(u_{00},u_{11},u_{01},u_{10})$, $\hat \mcE_c$:
% % 
% \begin{align}
%  \hat \mcE_c =& \left(H\oplus H\right)\hat \mcE \left(H\oplus H\right)^{\intercal}\\
%  =& \left(H\oplus H\right) \left(\tau_{03} \oplus \tau_{12}\right)\left(H\oplus H\right)^{\intercal}\\
%  =& H\tau_{03}H \oplus H\tau_{12}H\\
%  =&\begin{pmatrix}
%         \tau_0+\tau_3 & \tau_0-\tau_3\\
%         \tau_0-\tau_3 & \tau_0+\tau_3\\
%         &&\tau_0+\tau_3 & \tau_0-\tau_3\\
%         &&\tau_0-\tau_3 & \tau_0+\tau_3\\        
%     \end{pmatrix},
%  \label{Eq_E_P_C}
% \end{align}
% % 
% where $H$ is the Haddamard matrix
% % 
% \begin{equation}
%  H=\begin{pmatrix}
%         1 & 1\\
%         1 & -1\\
%     \end{pmatrix}.
%  \label{Hadd_Mat}
% \end{equation}
% % 
% After applying the reshuffling operation on $\hat \mcE_c$, we get the associated Choi matrix (or sometimes called dynamical matrix) of the map $D$, it reads
% % 
% \begin{equation}
%  D=\begin{pmatrix}
%         \tau_0+\tau_3 & \tau_1+\tau_2\\
%         \tau_1+\tau_2 & \tau_0+\tau_3\\
%         &&\tau_0-\tau_3 & \tau_1-\tau_2\\
%         &&\tau_1-\tau_2 & \tau_0-\tau_3\\        
%     \end{pmatrix}.
%  \label{Eq_D_P}
% \end{equation}
% % 
% Back to the Pauli basis, the Choi matrix holds
% \begin{align}
%  D_p =& \left(\frac{1}{2}H\oplus H\right)\hat D \left(\frac{1}{2}H\oplus H\right)^{\intercal}\\
%  =&\frac{1}{4}\left(H\oplus H\right)\hat D \left(H\oplus H\right)^{\intercal}\\
% =&\begin{pmatrix}
%         \lambda_0\\
%         &\lambda_1\\
%         &&\lambda_2\\
%         &&&\lambda_3\\
%     \end{pmatrix}.
% %   =&\begin{pmatrix}
% %         \tau_0+\tau_3+\tau_1+\tau_2\\
% %         &\tau_0-\tau_3+\tau_1-\tau_2\\
% %         &&\tau_0+\tau_3-\tau_1-\tau_2\\
% %         &&&\tau_0-\tau_3-\tau_1+\tau_2\\        
% %     \end{pmatrix}.
%  \label{Eq_D_C_P}
% \end{align}
% % 
% In other words, the Pauli basis diagonalizes the Choi matrix associated to any Pauli map, with eigenvalues related to the map coefficients $\tau$ as
% % 
% \begin{equation}
% \begin{pmatrix}
%         \lambda_0\\
%         \lambda_1\\
%         \lambda_2\\
%         \lambda_3\\   
%     \end{pmatrix}
%  =\frac{1}{2}\begin{pmatrix}
%         1&1&1&1\\
%         1&-1&1&-1\\
%         1&1&-1&-1\\
%         1&-1&-1&1\\
%     \end{pmatrix}
%     \begin{pmatrix}
%         \tau_0\\
%         \tau_3\\
%         \tau_1\\
%         \tau_2\\   
%     \end{pmatrix},
%  \label{Eigen1}
% \end{equation}
% % 
% or in a compact form
% % 
% \begin{equation}
%  \lambda = \frac{1}{2}H\otimes H \tau.
% \end{equation}
% % 
% Now we proceed to generalize this result for the case $N$ qubits case.

\subsubsection{$N$ qubits}



Coming soon...


\subsection{Parte de Francois}


We have the matrix 
\begin{equation}
\sa=\left(
\begin{array}{cccc}
1 &1 &1   &1   \\
 1 & 1  & -1&-1   \\
1  & -1  & 1 & -1\\
1&-1&-1&1  
\end{array}
\right)=\frac12\sa^{-1}.
\label{eq:1}
\end{equation}
and further define the matrix
\begin{equation}
\san=\left(\sa\right)^{\otimes N}
\label{eq:1a}
\end{equation}
% }}}
\subsection{A vector space of channels} % {{{
% \begin{itemize} % {{{ Cosas por escribir aca
% \item Canales PCE como espacios vectoriales \todo{Francois escribe}
% \item Regla de $2^k$
% \item Tamaño de las diferentes clases que borran un numero fijo de componentes
% y tambien nota de qeu son igual entre complementarios: computo del numero de
% bases dada una dimension de subespacios vectoriales, aqui entra la formula del
% binomial-q. \todo{Esperemos que evolucione mas el articulo y luego vemos que ponemos}
% \item Intento de identificar elementos, aka hermanistos PCE \cpnote{Esto dependerá de si sale algo interesante,}: De entrada aqui se tiene que mencionar minimo la identificacion del tamaño de las clases.
% \item \ddnote{Agrego esto:} A final remarks of the derivation as a tool.
% Concepts might be complicated, but the tool gives some simple rules to the
% reader: \textit{tell me what components you want to keep, and the (vector)
% summation rules will give you the rest of components to make the target
% operation a CPTP one.}
% \end{itemize} % }}}

The problem of determining complete positivity of a PCE can be recast as 
determining which vectors $\vec \tau$ are mapped via
$\san$
% \begin{equation}
% \san\vec\tau=2^N\vec p
% \label{eq:condition}
% \end{equation}
to a positive vector $\vec p$. In compontents, this would read 
\begin{equation}
\left(
\san_{\vec\alpha\vec\beta}
\right)
\tau_\beta=2^Np_{\vec\alpha}
\label{eq:4}
\end{equation}
where
$\san_{\vec\alpha\vec\beta} =\sa_{\alpha_1\beta_1}\cdot\ldots\cdot
\sa_{\alpha_N\beta_N}$. From the 



It results convenient to express the 
previous equation in  





We look for those vectors $\vec\tau$ consisting of 0's and 1's such that 
\begin{equation}
\san\vec\tau=2^N\vec p
\label{eq:3}
\end{equation}
where $\vec p$ is a vector with positive entries. Here $\vec\tau$ is a $4^N$ dimensional vector of 0's
and 1's. We number the entries of $\vec\tau$ 
by the multi-indices $\vec\alpha=(\alpha_1,\ldots,\alpha_N)$ where $0\leq\alpha_k\leq3$.
The entries of the positive vector are given by $2^Np_{\vec\alpha}$. It then follows that 
the expression (\ref{eq:3}) satisfies
\begin{equation}
\left(
\san_{\vec\alpha\vec\beta}
\right)
\tau_\beta=2^Np_{\vec\alpha}
\label{eq:4}
\end{equation}
where
\begin{equation}
\left(
\san_{\vec\alpha\vec\beta}
\right)=\sa_{\alpha_1\beta_1}\cdot\ldots\cdot \sa_{\alpha_N\beta_N}
\label{eq:5}
\end{equation}

From (\ref{eq:4}) immediately follows, via (\ref{eq:1})
\begin{equation}
\tau_\alpha=\left(
\san_{\vec\alpha\vec\beta}
\right)p_\beta
\label{eq:6}
\end{equation}
which we analyze iteratively. We shall essentially use the fact that
\begin{equation}
p_{\vec\alpha}+p_{\vec\beta}=0\quad\Leftrightarrow\quad p_{\vec\alpha}=p_{\vec\beta}=0
\label{eq:7}
\end{equation}

Now we need a definition: to each multi-index $\vec\alpha$ we associate a {\em set\/}
of multi-indices $\Phi(\vec\alpha)$ as follows
\begin{equation}
\Phi(\vec\alpha):=\left\{
\vec\beta:\san_{\vec\alpha\vec\beta}=1
\right\}=\left\{
\vec\beta:
\sa_{\alpha_1\beta_1}\cdot\ldots\cdot \sa_{\alpha_N\beta_N}
=1
\right\}
\label{eq:8}
\end{equation}
If we now assume that $\tau_{\vec\alpha}=1$, it follows from subtracting the two combined equations
\begin{subequations}
\begin{eqnarray}
\sum_{\vec\beta}p_{\vec\beta}&=&1
\label{eq:9a}\\
\sum_{\vec\beta}\san_{\vec\alpha\vec\beta}p_{\vec\beta}&=&\tau_{\vec\alpha}=1
\end{eqnarray}
\label{eq:9}
\end{subequations}
that for all $\vec\beta\notin\Phi(\vec\alpha)$
\begin{eqnarray}
p_{\vec\beta}=0.
\label{eq:10}
\end{eqnarray}
Thus, if $\tau_{\vec\alpha}=1$, then $\tau_{\vec\gamma}$
and $\tau_{\vec{\gamma^\prime}}$ are equal if, for all $\vec\beta\in\Phi(\vec\alpha)$.
\begin{equation}
\san_{\vec\beta\vec\gamma}=\san_{\vec\beta\vec{\gamma^\prime}}
\label{eq:11}
\end{equation}
holds for all $\vec\beta\in\Phi(\vec\alpha)$. This follows from (\ref{eq:6}) as well as the fact that, due 
to (\ref{eq:10}), we only need sum over $\vec\beta\in\Phi(\vec\alpha)$.


% If this condition is fulfilled and $\tau_{\vec\alpha}=1$, then
%\begin{subequations}
%\begin{eqnarray}
%\tau_{\vec\gamma}&=&\sum_{\vec\beta}\left(
%M^{\otimes N}
%\right)_{\vec\beta\vec\gamma}p_{\vec\beta}
%\label{eq:10na}
%\\
%&=&\sum_{\vec\beta\in\Phi(\vec\alpha)}\left(
%M^{\otimes N}
%\right)_{\vec\beta\vec\gamma}p_{\vec\beta}
%\label{eq:10nb}
%\\
%&=&\sum_{\vec\beta\in\Phi(\vec\alpha)}\left(
%M^{\otimes N}
%\right)_{\vec\beta\vec{\gamma^\prime}}p_{\vec\beta}
%\label{eq:10nc}
%\\
%&=&\sum_{\vec\beta}\left(
%M^{\otimes N}
%\right)_{\vec\beta\vec{\gamma^\prime}}p_{\vec\beta}
%\label{eq:10nd}
%\\
%&=&\tau_{\vec{\gamma^\prime}}
%\label{eq:10ne}
%\end{eqnarray}
%\label{eq:10n}
%\end{subequations}
%Here the transition from (\ref{eq:10na}) to (\ref{eq:10nb}) follows from (\ref{eq:10}), from (\ref{eq:10nb}) to (\ref{eq:10nc})
%follows from (\ref{eq:11}), from (\ref{eq:10nc}) to  (\ref{eq:10nd}) follows again from (\ref{eq:10}). 
  
Condition (\ref{eq:11}) therefore connects three multi-indinces, $\vec\alpha$, $\vec\gamma$ and $\vec{\gamma^\prime}$.
When such a connection exists, $\tau_{\vec\alpha}=1$ implies 
$\tau_{\vec\gamma}=\tau_{\vec{\gamma^\prime}}$. Let us now work out the nature of this connection. 

Since $\sa_{\alpha\beta}=\pm1$, we may rewrite (\ref{eq:11}) as
\begin{equation}
\sa_{\beta_1\gamma_1}\sa_{\beta_1\gamma_1^\prime}\cdot\ldots\cdot \sa_{\beta_N\gamma_N}\sa_{\beta_N\gamma_N^\prime}=1.
\label{eq:12}
\end{equation}
This must hold for all $\vec\beta\in\Phi(\vec\alpha)$. From this follows that (\ref{eq:12}) may be rewritten as
\begin{equation}
\sa_{\beta_1\gamma_1}\sa_{\beta_1\gamma_1^\prime}\cdot\ldots\cdot \sa_{\beta_N\gamma_N}\sa_{\beta_N\gamma_N^\prime}=
\sa_{\alpha_1\beta_1}\cdot\ldots\cdot \sa_{\alpha_N\beta_N}.
\label{eq:13}
\end{equation}
Since this must hold for all $\vec\beta\in\Phi(\vec\alpha)$, it follows that 
the above relation must hold individually for every index. Thus we have, 
that, for all $1\leq k\leq N$, $\gamma_k$
and $\gamma_k^\prime$ are connected by
\begin{equation}
\sa_{\beta\gamma_k}\sa_{\beta\gamma_k^\prime}=\sa_{\beta\alpha}
\label{eq:14}
\end{equation}
for all $0\leq\beta\leq3$. This is equivalently expressed as 
\begin{equation}
\gamma_k^\prime=\alpha\oplus{}\gamma_k
\label{eq:15}
\end{equation}
where the $\oplus{}$ operation is defined by (\ref{eq:14}). See Figure \ref{tab:1} for a detailed description. 

\begin{figure}
\begin{center}
\begin{tabular}{| c | c c c c |}
\hline
$\oplus{}$ & 0 & 1& 2 & 3\\
\hline
0 & 0 & 1 & 2 & 3\\
1 & 1 & 0 & 3 & 2\\
2 & 2 & 3 & 0 & 1 \\
3 &3 & 2 & 1 & 0\\
\hline
\end{tabular}
\caption{Table for the $\oplus{}$ operation defined in (\ref{eq:14}). Note that the operation
is an {\em abelian group}, in fact it corresponds to the {\em Klein group}, where the
neutral element is $0$. This is the reason for choosing an additive notation for the operation
defined in (\ref{eq:14}).
}
\label{tab:1}
\end{center}
\end{figure}

Note further that if we perform the identification
\begin{subequations}
\begin{eqnarray}
0&\to&(0,0)\label{eq:identa}\\
1&\to&(0,1)\label{eq:identb}\\
2&\to&(1,0)\label{eq:identc}\\
3&\to&(1,1)\label{eq:identd}
\end{eqnarray}
\label{eq:ident}
\end{subequations}
then the $\oplus$ operation reduces to simple vector addition in
binary arithmetic. If we therefore denote by $\overline{\alpha}$
the multi-index $\vec\alpha$ converted to a binary vector by the componentwise application of (\ref{eq:ident}),
the connection between $\vec\alpha$, $\vec\gamma$, and $\vec{\gamma^\prime}$ defined by (\ref{eq:11}) is rewritten 
as
\begin{equation}
\overline\gamma=\overline\alpha+\overline{\gamma^\prime}
\label{eq:17}
\end{equation}
where the addition sign here refers to ordinary binary vector addition. 

From this readily follows an amusing property: the set of all multi-indices $\overline\gamma$
for which $\tau_{\overline\gamma}=1$\cpnote{I guess instead of overline, we want 
to use arrow, or this denotes something else?}, is closed under binary vector addition, in other words, it
forms a {\em vector subspace\/} of the set of all multi-indices. Indeed, let
$\overline\alpha$ and $\overline\gamma$ belong to that set. Then if 
\begin{equation}
\overline{\gamma^\prime}=\overline\alpha+\overline\gamma
\label{eq:14b}
\end{equation}
then by the preceding considerations, $\tau_{\overline\gamma^\prime}$ is equal to $\tau_{\overline\gamma}$
and thus equal to one. 

So we may now proceed to generate all solutions: we start out from the solution having $\tau_{\vec0}=1$, with
everything else $0$. We may then successively switch $\tau$'s to $1$ for various values of $\vec\alpha$, 
taking care immediately to set equal to one the values of $\tau$ that correspond to values of $\vec\beta$
generated by the previously switched values of $\vec\alpha$ via (\ref{eq:11}). 


A moment's consideration will further show that the above reasoning can be inverted, that is, that if we
set all $\tau_{\overline\gamma}$ equal to one whenever the $\overline\gamma$ belong to a given vector 
subspace of the set of all indices, then the vector $\vec\tau$ indeed has an image which is a vector with 
only positive components. 


%Now everything follows with delightful simplicity. First extend the $\oplus{}$ operation to vectors 
%componentwise
%\begin{equation}
%\vec\alpha\oplus{}\vec\beta=(\alpha_1\oplus{}\beta_1,\ldots,\alpha_N\oplus{}\beta_N).
%\label{eq:16}
%\end{equation}
%Remark, as a useful fact, that the inverse under $\oplus$ of any number $\alpha$ is $\alpha$ itself. 

%If $\tau_{\vec\alpha}=1$, then it follows from the above that 
%for all $\gamma$
%\begin{equation}
%\tau_{\vec\gamma}=\tau_{\vec\alpha\oplus{}\vec\gamma}.
%\label{eq:17}
%\end{equation}

%Now take an arbitrary $\vec{\alpha}_1$ and set $\tau_{\vec{\alpha}_1}=1$. This immediately imposes the
%relationship
%\begin{equation}
%\tau_{\vec\gamma}=\tau_{\alp1\oplus{}\vec\gamma}
%\label{eq:18}
%\end{equation}
%which to every $\vec\gamma$ assigns exactly one other multi-index such that the corresponding values of $\tau$
%must be equal. Note that (\ref{eq:18}) is vacuous when applied to $\gamma=\alp1$: indeed $\alp1\oplus\alp1=\vec0$,
%and $\tau_{\vec0}$ is one by convention. 
%
%Now choose another multi-index, say $\alp2$, which is not equal to $\alp1$, nor to any site
%that is automatically equal to $\tau_{\alp1}$. Set $\tau_{\alp2}=1$. We obviously must also set $\tau_{\alp2\oplus\alp1}=1$,
%since the two are equal because of (\ref{eq:18}). By the same reasoning that led to (\ref{eq:18}), we now find that 
%\begin{equation}
%\tau_{\vec\gamma}=\tau_{\alp1\oplus{}\vec\gamma}=\tau_{\alp2\oplus{}\vec\gamma}=\tau_{\alp1\oplus\alp2\alp2\oplus{}\vec\gamma}
%\label{eq:19}
%\end{equation}
%Thus we find that the $\tau$'s are now constant over domains of $4$, as opposed to $2$ previously, when only one
%single $\vec\alpha$, namely $\alp1$, had been fixed to have $\tau_{\alp1}=1$. Further, it follows from the group property
%of $\oplus$ that the different domains are {\em disjoint}. The 3 multi-indices associated to a given $\vec\gamma$ are never associated
%to any other $\vec{\gamma^\prime}$. 
%
%After adding $\alp1$, we have at least 2 sites equal to 1, after adding $\alp2$, we have 4: $\vec0$, $\alp1$, $\alp2$, and $\alp1+\alp2$. 
%Clearly, at each addition, the total number doubles. This shows the $2^k$ rule. 
%
%How far can this process go? Well, the total number of multi-indices is $4^N$, so this can go on until $2\cdot4^{N-1}$ sites are occupied,
%in other words, we may double $2N-1$ times.
%
%Now to show that the maximal solutions are generators, that is, that all solutions can be expressed as intersections of maximal 
%solutions, that is, solutions with $2\cdot4^{N-1}$ 1's. Any maximal solution can be obtained by the successive adjunction
%of $\alp1, \alp2,\ldots,\alp{2N-1}$. Again, due to the group property, the final solution generated does not depend on the order
%of the $\alp{k}$. 
%
%Now let us consider a solution with half the number of elements of a maximal solution, that is, one generated by $\alp1, \alp2,\ldots,\alp{2N-2}$. 
%At this stage, the $\tau_{\vec\alpha}$ are constant over domains of size $4^{N-1}$ and there are a total of $4^{N-1}$ 1's. 
%There are thus 2 inequivalent remaining domains where I could put the final $\alp{2N-1}$. These 2 different choices yield disjoint
%maximal solutions, the intersection of which yields the solution with half the number of elements of the maximal solution. 
%Every solution one step away from maximal can thus be expressed (uniquely) as the intersection of 2 maximal solutions. 
%
%Arguing inductively backwards, the same result can be shown for arbitrary solutions. We therefore have shown both the $2^k$ 
%rule and the fact that all solutions are expressible uniquely as intersections of maximal solutions. 

The dimension of the vector space $V$ of all multi-indices is $2N$, and we have seen earlier that
\begin{equation}
W=\left\{
\vec\alpha:\vec\alpha\in V, \tau_{\vec\alpha}=1
\right\}
\label{eq:19}
\end{equation}
is a subspace of $V$. As such, $W$ has a given dimension $K$, which means that $W$ has $2^K$ elements. In other words,
a vector $\vec\tau$ with the property discussed above can only have $2^K$ elements equal to 1, for a given integer $K$.

By standard theorems of linear algebra, any subspace $W$ can be extended to a maximal (non-trivial) subspace of dimension
$2N-1$ by adjoining appropriate additional basis elements. This can clearly be done in different ways. We therefore arrive to
the set of maximal extensions of $W$. Clearly, the intersection of all the elements of this set reduces to $W$ itself, leading to
the result that all PCE's can be obtained as intersections of maximal PCE's. 



\ddnote{Agrego esto:} A final remarks of the derivation as a tool. Concepts might be complicated, but the tool gives some simple rules to the reader: \textit{tell me what components you want to keep, and the (vector) summation rules will give you the rest of components to make the target operation a CPTP one.}

% }}}


