\section{Generators} % {{{
\todo{Francois escribe una primera version del primer punto, y luego quizá Jose
Alfredo o Carlos la terminan}


We have the matrix 
\begin{equation}
\sa=\left(
\begin{array}{cccc}
1 &1 &1   &1   \\
 1 & 1  & -1&-1   \\
1  & -1  & 1 & -1\\
1&-1&-1&1  
\end{array}
\right)=\frac12\sa^{-1}.
\label{eq:1}
\end{equation}
and further define the matrix
\begin{equation}
\san=\left(\sa\right)^{\otimes N}
\label{eq:1a}
\end{equation}
We look for those vectors $\vec\sigma$ consisting of 0's and 1's such that 
\begin{equation}
\san\vec\sigma=2^N\vec p
\label{eq:3}
\end{equation}
where $\vec p$ is a vector with positive entries. Here $\vec\sigma$ is a $4^N$ dimensional vector of 0's
and 1's. We number the entries of $\vec\sigma$ 
by the multi-indices $\vec\alpha=(\alpha_1,\ldots,\alpha_N)$ where $0\leq\alpha_k\leq4$.
The entries of the positive vector are given by $2^Np_{\vec\alpha}$. It then follows that 
the expression (\ref{eq:3}) satisfies
\begin{equation}
\left(
\san_{\vec\alpha\vec\beta}
\right)
\sigma_\beta=2^Np_{\vec\alpha}
\label{eq:4}
\end{equation}
where
\begin{equation}
\left(
\san_{\vec\alpha\vec\beta}
\right)=\sa_{\alpha_1\beta_1}\cdot\ldots\cdot \sa_{\alpha_N\beta_N}
\label{eq:5}
\end{equation}

From (\ref{eq:4}) immediately follows, via (\ref{eq:1})
\begin{equation}
\sigma_\alpha=\left(
\san_{\vec\alpha\vec\beta}
\right)p_\beta
\label{eq:6}
\end{equation}
which we analyze iteratively. We shall essentially use the fact that
\begin{equation}
p_{\vec\alpha}+p_{\vec\beta}=0\quad\Leftrightarrow\quad p_{\vec\alpha}=p_{\vec\beta}=0
\label{eq:7}
\end{equation}

Now we need a definition: to each multi-index $\vec\alpha$ we associate a {\em set\/}
of multi-indices $\Phi(\vec\alpha)$ as follows
\begin{equation}
\Phi(\vec\alpha):=\left\{
\vec\beta:\san_{\vec\alpha\vec\beta}=1
\right\}=\left\{
\vec\beta:
\sa_{\alpha_1\beta_1}\cdot\ldots\cdot \sa_{\alpha_N\beta_N}
=1
\right\}
\label{eq:8}
\end{equation}
If we now assume that $\sigma_{\vec\alpha}=1$, it follows from subtracting the two combined equations
\begin{subequations}
\begin{eqnarray}
\sum_{\vec\beta}p_{\vec\beta}&=&1
\label{eq:9a}\\
\sum_{\vec\beta}\san_{\vec\alpha\vec\beta}p_{\vec\beta}&=&\sigma_{\vec\alpha}=1
\end{eqnarray}
\label{eq:9}
\end{subequations}
that for all $\vec\beta\notin\Phi(\vec\alpha)$
\begin{eqnarray}
p_{\vec\beta}=0.
\label{eq:10}
\end{eqnarray}
Thus, if $\sigma_{\vec\alpha}=1$, then $\sigma_{\vec\gamma}$
and $\sigma_{\vec{\gamma^\prime}}$ are equal if, for all $\vec\beta\in\Phi(\vec\alpha)$.
\begin{equation}
\san_{\vec\beta\vec\gamma}=\san_{\vec\beta\vec{\gamma^\prime}}
\label{eq:11}
\end{equation}
holds for all $\vec\beta\in\Phi(\vec\alpha)$. This follows from (\ref{eq:6}) as well as the fact that, due 
to (\ref{eq:10}), we only need sum over $\vec\beta\in\Phi(\vec\alpha)$.


% If this condition is fulfilled and $\sigma_{\vec\alpha}=1$, then
%\begin{subequations}
%\begin{eqnarray}
%\sigma_{\vec\gamma}&=&\sum_{\vec\beta}\left(
%M^{\otimes N}
%\right)_{\vec\beta\vec\gamma}p_{\vec\beta}
%\label{eq:10na}
%\\
%&=&\sum_{\vec\beta\in\Phi(\vec\alpha)}\left(
%M^{\otimes N}
%\right)_{\vec\beta\vec\gamma}p_{\vec\beta}
%\label{eq:10nb}
%\\
%&=&\sum_{\vec\beta\in\Phi(\vec\alpha)}\left(
%M^{\otimes N}
%\right)_{\vec\beta\vec{\gamma^\prime}}p_{\vec\beta}
%\label{eq:10nc}
%\\
%&=&\sum_{\vec\beta}\left(
%M^{\otimes N}
%\right)_{\vec\beta\vec{\gamma^\prime}}p_{\vec\beta}
%\label{eq:10nd}
%\\
%&=&\sigma_{\vec{\gamma^\prime}}
%\label{eq:10ne}
%\end{eqnarray}
%\label{eq:10n}
%\end{subequations}
%Here the transition from (\ref{eq:10na}) to (\ref{eq:10nb}) follows from (\ref{eq:10}), from (\ref{eq:10nb}) to (\ref{eq:10nc})
%follows from (\ref{eq:11}), from (\ref{eq:10nc}) to  (\ref{eq:10nd}) follows again from (\ref{eq:10}). 
  
Condition (\ref{eq:11}) therefore connects three multi-indinces, $\vec\alpha$, $\vec\gamma$ and $\vec{\gamma^\prime}$.
When such a connection exists, $\sigma_{\vec\alpha}=1$ implies 
$\sigma_{\vec\gamma}=\sigma_{\vec{\gamma^\prime}}$. Let us now work out the nature of this connection. 

Since $\sa_{\alpha\beta}=\pm1$, we may rewrite (\ref{eq:11}) as
\begin{equation}
\sa_{\beta_1\gamma_1}\sa_{\beta_1\gamma_1^\prime}\cdot\ldots\cdot \sa_{\beta_N\gamma_N}\sa_{\beta_N\gamma_N^\prime}=1.
\label{eq:12}
\end{equation}
This must hold for all $\vec\beta\in\Phi(\vec\alpha)$. From this follows that (\ref{eq:12}) may be rewritten as
\begin{equation}
\sa_{\beta_1\gamma_1}\sa_{\beta_1\gamma_1^\prime}\cdot\ldots\cdot \sa_{\beta_N\gamma_N}\sa_{\beta_N\gamma_N^\prime}=
\sa_{\alpha_1\beta_1}\cdot\ldots\cdot \sa_{\alpha_N\beta_N}.
\label{eq:13}
\end{equation}
Since this must hold for all $\vec\beta\in\Phi(\vec\alpha)$, it follows that 
the above relation must hold individually for every index. Thus we have, 
that, for all $1\leq k\leq N$, $\gamma_k$
and $\gamma_k^\prime$ are connected by
\begin{equation}
\sa_{\beta\gamma_k}\sa_{\beta\gamma_k^\prime}=\sa_{\beta\alpha}
\label{eq:14}
\end{equation}
for all $0\leq\beta\leq3$. This is equivalently expressed as 
\begin{equation}
\gamma_k^\prime=\alpha\oplus{}\gamma_k
\label{eq:15}
\end{equation}
where the $\oplus{}$ operation is defined by (\ref{eq:14}). See Figure \ref{tab:1} for a detailed description. 

\begin{figure}
\begin{center}
\begin{tabular}{| c | c c c c |}
\hline
$\oplus{}$ & 0 & 1& 2 & 3\\
\hline
0 & 0 & 1 & 2 & 3\\
1 & 1 & 0 & 3 & 2\\
2 & 2 & 3 & 0 & 1 \\
3 &3 & 2 & 1 & 0\\
\hline
\end{tabular}
\caption{Table for the $\oplus{}$ operation defined in (\ref{eq:14}). Note that the operation
is an {\em abelian group}, in fact it corresponds to the {\em Klein group}, where the
neutral element is $0$. This is the reason for choosing an additive notation for the operation
defined in (\ref{eq:14}).
}
\label{tab:1}
\end{center}
\end{figure}

Note further that if we perform the identification
\begin{subequations}
\begin{eqnarray}
0&\to&(0,0)\label{eq:identa}\\
1&\to&(0,1)\label{eq:identb}\\
2&\to&(1,0)\label{eq:identc}\\
3&\to&(1,1)\label{eq:identd}
\end{eqnarray}
\label{eq:ident}
\end{subequations}
then the $\oplus$ operation reduces to simple vector addition in
binary arithmetic. If we therefore denote by $\overline{\alpha}$
the multi-index $\vec\alpha$ converted to a binary vector by the componentwise application of (\ref{eq:ident}),
the connection between $\vec\alpha$, $\vec\gamma$, and $\vec{\gamma^\prime}$ defined by (\ref{eq:11}) is rewritten 
as
\begin{equation}
\overline\gamma=\overline\alpha+\overline{\gamma^\prime}
\label{eq:17}
\end{equation}
where the addition sign here refers to ordinary binary vector addition. 

From this readily follows an amusing property: the set of all multi-indices $\overline\gamma$
for which $\sigma_{\overline\gamma}=1$, is closed under binary vector addition, in other words, it
forms a {\em vector subspace\/} of the set of all multi-indices. Indeed, let
$\overline\alpha$ and $\overline\gamma$ belong to that set. Then if 
\begin{equation}
\overline{\gamma^\prime}=\overline\alpha+\overline\gamma
\label{eq:14b}
\end{equation}
then by the preceding considerations, $\sigma_{\overline\gamma^\prime}$ is equal to $\sigma_{\overline\gamma}$
and thus equal to one. 

So we may now proceed to generate all solutions: we start out from the solution having $\sigma_{\vec0}=1$, with
everything else $0$. We may then successively switch $\sigma$'s to $1$ for various values of $\vec\alpha$, 
taking care immediately to set equal to one the values of $\sigma$ that correspond to values of $\vec\beta$
generated by the previously switched values of $\vec\alpha$ via (\ref{eq:11}). 


A moment's consideration will further show that the above reasoning can be inverted, that is, that if we
set all $\sigma_{\overline\gamma}$ equal to one whenever the $\overline\gamma$ belong to a given vector 
subspace of the set of all indices, then the vector $\vec\sigma$ indeed has an image which is a vector with 
only positive components. 


%Now everything follows with delightful simplicity. First extend the $\oplus{}$ operation to vectors 
%componentwise
%\begin{equation}
%\vec\alpha\oplus{}\vec\beta=(\alpha_1\oplus{}\beta_1,\ldots,\alpha_N\oplus{}\beta_N).
%\label{eq:16}
%\end{equation}
%Remark, as a useful fact, that the inverse under $\oplus$ of any number $\alpha$ is $\alpha$ itself. 

%If $\sigma_{\vec\alpha}=1$, then it follows from the above that 
%for all $\gamma$
%\begin{equation}
%\sigma_{\vec\gamma}=\sigma_{\vec\alpha\oplus{}\vec\gamma}.
%\label{eq:17}
%\end{equation}

%Now take an arbitrary $\vec{\alpha}_1$ and set $\sigma_{\vec{\alpha}_1}=1$. This immediately imposes the
%relationship
%\begin{equation}
%\sigma_{\vec\gamma}=\sigma_{\alp1\oplus{}\vec\gamma}
%\label{eq:18}
%\end{equation}
%which to every $\vec\gamma$ assigns exactly one other multi-index such that the corresponding values of $\sigma$
%must be equal. Note that (\ref{eq:18}) is vacuous when applied to $\gamma=\alp1$: indeed $\alp1\oplus\alp1=\vec0$,
%and $\sigma_{\vec0}$ is one by convention. 
%
%Now choose another multi-index, say $\alp2$, which is not equal to $\alp1$, nor to any site
%that is automatically equal to $\sigma_{\alp1}$. Set $\sigma_{\alp2}=1$. We obviously must also set $\sigma_{\alp2\oplus\alp1}=1$,
%since the two are equal because of (\ref{eq:18}). By the same reasoning that led to (\ref{eq:18}), we now find that 
%\begin{equation}
%\sigma_{\vec\gamma}=\sigma_{\alp1\oplus{}\vec\gamma}=\sigma_{\alp2\oplus{}\vec\gamma}=\sigma_{\alp1\oplus\alp2\alp2\oplus{}\vec\gamma}
%\label{eq:19}
%\end{equation}
%Thus we find that the $\sigma$'s are now constant over domains of $4$, as opposed to $2$ previously, when only one
%single $\vec\alpha$, namely $\alp1$, had been fixed to have $\sigma_{\alp1}=1$. Further, it follows from the group property
%of $\oplus$ that the different domains are {\em disjoint}. The 3 multi-indices associated to a given $\vec\gamma$ are never associated
%to any other $\vec{\gamma^\prime}$. 
%
%After adding $\alp1$, we have at least 2 sites equal to 1, after adding $\alp2$, we have 4: $\vec0$, $\alp1$, $\alp2$, and $\alp1+\alp2$. 
%Clearly, at each addition, the total number doubles. This shows the $2^k$ rule. 
%
%How far can this process go? Well, the total number of multi-indices is $4^N$, so this can go on until $2\cdot4^{N-1}$ sites are occupied,
%in other words, we may double $2N-1$ times.
%
%Now to show that the maximal solutions are generators, that is, that all solutions can be expressed as intersections of maximal 
%solutions, that is, solutions with $2\cdot4^{N-1}$ 1's. Any maximal solution can be obtained by the successive adjunction
%of $\alp1, \alp2,\ldots,\alp{2N-1}$. Again, due to the group property, the final solution generated does not depend on the order
%of the $\alp{k}$. 
%
%Now let us consider a solution with half the number of elements of a maximal solution, that is, one generated by $\alp1, \alp2,\ldots,\alp{2N-2}$. 
%At this stage, the $\sigma_{\vec\alpha}$ are constant over domains of size $4^{N-1}$ and there are a total of $4^{N-1}$ 1's. 
%There are thus 2 inequivalent remaining domains where I could put the final $\alp{2N-1}$. These 2 different choices yield disjoint
%maximal solutions, the intersection of which yields the solution with half the number of elements of the maximal solution. 
%Every solution one step away from maximal can thus be expressed (uniquely) as the intersection of 2 maximal solutions. 
%
%Arguing inductively backwards, the same result can be shown for arbitrary solutions. We therefore have shown both the $2^k$ 
%rule and the fact that all solutions are expressible uniquely as intersections of maximal solutions. 

The dimension of the vector space $V$ of all multi-indices is $2N$, and we have seen earlier that
\begin{equation}
W=\left\{
\vec\alpha:\vec\alpha\in V, \sigma_{\vec\alpha}=1
\right\}
\label{eq:19}
\end{equation}
is a subspace of $V$. As such, $W$ has a given dimension $K$, which means that $W$ has $2^K$ elements. In other words,
a vector $\vec\sigma$ with the property discussed above can only have $2^K$ elements equal to 1, for a given integer $K$.

By standard theorems of linear algebra, any subspace $W$ can be extended to a maximal (non-trivial) subspace of dimension
$2N-1$ by adjoining appropriate additional basis elements. This can clearly be done in different ways. We therefore arrive to
the set of maximal extensions of $W$. Clearly, the intersection of all the elements of this set reduces to $W$ itself, leading to
the result that all PCE's can be obtained as intersections of maximal PCE's. 

****************************************

This should probably be in an Appendix

Finally, we may enumerate straightforwardly the subspaces $W$ of dimension $K$. We do this in 2 steps: first, we evaluate
$\nnn_{K,N}$, the number of all linearly independent subsets $V$ with $K$ elements. Each of these is the basis of one 
subspace of dimension $K$, but 
each subspace has a number $\mmm_K$ of different bases. The crucial point is that $\mmm_K$ is independent 
of the subspace under consideration: $\mmm_K$ simply describes the number of linear maps of $W$ onto itself. The 
total number $\sss_{N,K}$ of subspaces of dimension $K$ is therefore $\nnn_{N,K}/\mmm_K$. 

To evaluate $\nnn_{N,K}$ we proceed by steps: the first element of the basis can be any non-zero element, of which the number
is $2^{2N}-1$. For the  basis element $m+1$, we must choose from those which do not belong to the 
$m$ dimensional space generated by the first $m$ basis elements, so that one chooses from $2^{2N}-2^{m}$. We thus have
\begin{equation}
\nnn_{N,K}=\prod_{m=0}^{K-1}\left(
2^{2N}-2^{m}
\right).
\label{eq.20}
\end{equation}
On the other hand, any map of a $K$-dimensional vector space $W$ onto itself is uniquely
defined by a non-singular binary $K\times K$ matrix over the field $\{0,1\}$. To count these, we proceed as 
above: the first line is an arbitrary non-zero vector, of which there are $2^K-1$. For the row $m+1$  we must 
choose an arbitrary vector not belonging to those generated by the first $m$ vectors, of which there are $2^K-2^{m}$. 
This eventually yields
\begin{equation}
\mmm_{K}=\prod_{m=0}^{K-1}\left(
2^{K}-2^{m}
\right).
\label{eq.21}
\end{equation}
From this follows that
\begin{equation}
\sss_{N,K}=\prod_{m=0}^{K-1}
\frac{2^{2N-m}-1}{2^{K-m}-1}
.
\label{eq:22}
\end{equation}
%An elementary test is $N=3$ and $K=2,3$: 
%\begin{eqnarray}
%\sss_{3,2}&=&\frac{(2^6-1)(2^5-1)}{(2^2-1)(2^1-1)}=\frac{63\cdot31}{3}=651,\\
%\sss_{3,3}&=&\frac{(2^6-1)(2^5-1)(2^4-1)}{(2^3-1)(2^2-1)(2^1-1)}=\frac{63\cdot31\cdot15}{7\cdot3}=1395,
%\end{eqnarray}
%which are indeed the values found numerically. 
The following symmetry relation
\begin{equation}
\sss_{N,K}=\sss_{N,2N-K}
\end{equation}
can also be proved straightforwardly from (\ref{eq:22}).


****************************************


\begin{itemize}
\item Clasificacion de los generadores, es decir que para dos qubits, lo podemos indicar con la acción sobre cada qubit individual
\item Quizá mostrar algunas propiedades de simetrías. 
\item Notar lo de los generadores y como sale la estructura jerárquica 
\end{itemize}
% }}}


